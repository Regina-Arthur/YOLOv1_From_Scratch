{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Regina-Arthur/YOLOv1_From_Scratch/blob/main/YOLOv1_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building a YOLOv1 model from scratch by Regina Arthur\n"
      ],
      "metadata": {
        "id": "E075aKobGM0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This will be done without the convolutional layers being pretrained on imagenet"
      ],
      "metadata": {
        "id": "2C124Tg5bKL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import the necessary libraries\n"
      ],
      "metadata": {
        "id": "B97rFjkKGMf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZcJyo6t5oNmj"
      },
      "outputs": [],
      "source": [
        "#Let's import the necessary libraries to build YOLOv1\n",
        "#with pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms.v2 as v2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Connect To a GPU if available"
      ],
      "metadata": {
        "id": "OfJGkqRgF28o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check if cuda is available and connect to it.\n",
        "#If it is not available, use cpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kATyTVUZpBC2",
        "outputId": "964478e7-fda9-400c-ec9d-c1073b4a524b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the Semantic Boundaries Dataset\n"
      ],
      "metadata": {
        "id": "X2eTGSsJOdtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a folder to store the Semantic Boundaries Dataset"
      ],
      "metadata": {
        "id": "PMsa5er5doKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Let's import the necessary libraries for creating a folder in google drive\n",
        "# from google.colab import drive\n",
        "# import os\n",
        "\n",
        "# #Let's mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# def determinedirectory(directory):\n",
        "#   #Let's define folder path in Google Drive\n",
        "#   location = directory.upper()\n",
        "#   root ='/content/drive/MyDrive/_Dataset/'\n",
        "#   new_root = root + location\n",
        "#   #Let's check if the folder exists, if not, let's create it\n",
        "#   if not os.path.exists(new_root):\n",
        "#       os.makedirs(new_root)\n",
        "#       print(f\"Created new folder: {new_root}\")\n",
        "#   else:\n",
        "#       print(f\"Folder already exists: {new_root}\")\n",
        "#   return new_root\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hG5Argrdmr1",
        "outputId": "4499c06b-f679-45a5-c280-b7ddfb01d761"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -q kaggle"
      ],
      "metadata": {
        "id": "pT3cYLdDn-NT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# !kaggle datasets download -d vijayabhaskar96/pascal-voc-2007-and-2012\n",
        "\n",
        "\n",
        "# output_dir = determinedirectory(\"PascalVOC2007_and_2012\")\n",
        "\n",
        "\n",
        "# !unzip \"pascal-voc-2007-and-2012.zip\" -d \"$output_dir\""
      ],
      "metadata": {
        "id": "FDAruwWIvDAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Network Architecture\n"
      ],
      "metadata": {
        "id": "ZoJTVgTjpDHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###I am following the documentation on the MODULE class to build my neural network. All neural network model are to inherit from the nn.Module class.\n",
        "\n",
        "####Conv2d\n",
        "####class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "\n",
        "####class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
        "\n",
        "####MaxPool2d\n",
        "####class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)[source]\n",
        "\n",
        "####Linear\n",
        "####class torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)[source]\n",
        "\n",
        "####Dropout2d\n",
        "####class torch.nn.Dropout2d(p=0.5, inplace=False)[source]\n",
        "\n"
      ],
      "metadata": {
        "id": "l3MISm9aqDlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOv1(nn.Module):\n",
        "  def __init__(self, S=7, B=2, C=20):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels = 3,out_channels = 64, kernel_size = 7,stride = 2, padding = 3)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 64,out_channels = 192, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 192,out_channels = 128, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv4 = nn.Conv2d(in_channels = 128,out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv5 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 1, stride = 1, padding = 0)\n",
        "    self.conv6 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv7 = nn.Conv2d(in_channels = 512, out_channels = 256, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv8 = nn.Conv2d(in_channels = 256,out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv9 = nn.Conv2d(in_channels = 512,out_channels = 256, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv10 = nn.Conv2d(in_channels = 256,out_channels = 512, kernel_size = 3,stride = 1, padding =1)\n",
        "    self.conv11 = nn.Conv2d(in_channels = 512,out_channels = 256, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv12 = nn.Conv2d(in_channels = 256,out_channels = 512, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv13 = nn.Conv2d(in_channels = 512,out_channels = 256, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv14 = nn.Conv2d(in_channels = 256,out_channels = 512, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv15 = nn.Conv2d(in_channels = 512,out_channels = 512, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv16 = nn.Conv2d(in_channels = 512,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv17 = nn.Conv2d(in_channels = 1024,out_channels = 512, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv18 = nn.Conv2d(in_channels = 512,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv19 = nn.Conv2d(in_channels = 1024,out_channels = 512, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv20 = nn.Conv2d(in_channels = 512,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv21 = nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv22 = nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3,stride = 2, padding = 1)\n",
        "    self.conv23 = nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv24 = nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "\n",
        "    #Pooling and Activation\n",
        "    self.MaxPooling2d = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    self.LeakyReLU = nn.LeakyReLU(negative_slope = 0.1)\n",
        "\n",
        "    #Flattening and Fully Connected Layer\n",
        "    self.Flatten = nn.Flatten()\n",
        "    self.Linear = nn.Linear(in_features = 50176 , out_features = 4096)\n",
        "    self.Dropout = nn.Dropout2d(0.5)\n",
        "    self.Linear2 = nn.Linear(in_features =4096 , out_features = S * S *( B * 5 + 20))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.MaxPooling2d(self.LeakyReLU(self.conv1(x)))\n",
        "    x = self.MaxPooling2d(self.LeakyReLU(self.conv2(x)))\n",
        "    x = self.LeakyReLU(self.conv3(x))\n",
        "    x = self.LeakyReLU(self.conv4(x))\n",
        "    x = self.LeakyReLU(self.conv5(x))\n",
        "    x = self.MaxPooling2d(self.LeakyReLU(self.conv6(x)))\n",
        "    x = self.LeakyReLU(self.conv7(x))\n",
        "    x = self.LeakyReLU(self.conv8(x))\n",
        "    x = self.LeakyReLU(self.conv9(x))\n",
        "    x = self.LeakyReLU(self.conv10(x))\n",
        "    x = self.LeakyReLU(self.conv11(x))\n",
        "    x = self.LeakyReLU(self.conv12(x))\n",
        "    x = self.LeakyReLU(self.conv13(x))\n",
        "    x = self.LeakyReLU(self.conv14(x))\n",
        "    x = self.LeakyReLU(self.conv15(x))\n",
        "    x = self.MaxPooling2d(self.LeakyReLU(self.conv16(x)))\n",
        "    x = self.LeakyReLU(self.conv17(x))\n",
        "    x = self.LeakyReLU(self.conv18(x))\n",
        "    x = self.LeakyReLU(self.conv19(x))\n",
        "    x = self.LeakyReLU(self.conv20(x))\n",
        "    x = self.LeakyReLU(self.conv21(x))\n",
        "    x = self.LeakyReLU(self.conv22(x))\n",
        "    x = self.LeakyReLU(self.conv23(x))\n",
        "    x = self.LeakyReLU(self.conv24(x))\n",
        "    x = self.LeakyReLU(self.Linear(self.Flatten(x)))\n",
        "    x = self.Dropout(x)\n",
        "    x = self.Linear2(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "zjf2zbBvpME-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture Test\n",
        "This just makes sure the architecture works as intended\n"
      ],
      "metadata": {
        "id": "PBdPPz0vH-th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = YOLOv1()\n",
        "# model = model.to(device)"
      ],
      "metadata": {
        "id": "cwXfGwm1IVwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy_input = torch.randn(1, 3, 448, 448).to(device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     output = model(dummy_input)\n",
        "\n",
        "# print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2tvjBG5KNa5",
        "outputId": "f99f3132-f9f6-4ce8-dfc0-398b45d9c2df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 1470])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:1535: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# total_params = sum(p.numel() for p in model.parameters())\n",
        "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# print(f\"Total params: {total_params:,}\")\n",
        "# print(f\"Trainable params: {trainable_params:,}\")"
      ],
      "metadata": {
        "id": "kAtjUMCyKkv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a5f213-8ae2-4df9-cd61-4fe3ef2925ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 271,703,550\n",
            "Trainable params: 271,703,550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOv1Loss(nn.Module):\n",
        "  def __init__(self, S=7, B=2, C=20, λ_coord=5, λ_noobj=0.5):\n",
        "    super().__init__()"
      ],
      "metadata": {
        "id": "UvbjN5aqQkFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the Pascal Visual Object Class Dataset"
      ],
      "metadata": {
        "id": "WdEdF9L5Orvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "model = YOLOv1()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "RJdBbpI3Wf09"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLlkieRy_KEg",
        "outputId": "43b44a56-02f3-4a14-b14a-a4022272d334"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's reshape the images\n",
        "transforms = v2.Compose([\n",
        "    v2.ColorJitter(brightness=1.5, contrast=1.5, saturation=1.5),\n",
        "    v2.RandomHorizontalFlip(p=0.2),\n",
        "    v2.RandomVerticalFlip(p=0.2),\n",
        "    v2.Resize((448,448)),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    ])\n",
        "\n",
        "TrainData = torchvision.datasets.VOCDetection(\n",
        "    root = '/content/drive/MyDrive/_Dataset/PASCALVOC2007_AND_2012',\n",
        "    year = '2007',\n",
        "    image_set = 'train',\n",
        "    download = False,\n",
        "    transform = transforms,\n",
        "    )\n",
        "\n",
        "\n",
        "Traindataloader = DataLoader(TrainData,\n",
        "                             batch_size=64,\n",
        "                             shuffle=True,\n",
        "                             num_workers=2,\n",
        "                             )"
      ],
      "metadata": {
        "id": "4HFih5GnlV2P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###AdamW\n",
        "class torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False, *, maximize=False, foreach=None, capturable=False, differentiable=False, fused=None)\n"
      ],
      "metadata": {
        "id": "fsgKHPV3VGV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_rate(epoch, epochs):\n",
        "  first_stop = 0.6 * epochs\n",
        "  second_stop = 0.8 * epochs\n",
        "  if epoch <= first_stop:\n",
        "    return 0.1\n",
        "  elif epoch <= second_stop:\n",
        "    return 0.01\n",
        "  else:\n",
        "    return 0.001\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr= 0.1,\n",
        "                              betas=(0.9,0.99),\n",
        "                              weight_decay= 0.0005,\n",
        "                              )\n",
        "loss = YOLOv1Loss()\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "vPCDKZ1PVCtD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.param_groups[0]['lr'] = learning_rate(epoch, num_epochs)\n",
        "    for images, targets in Traindataloader:\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "BNJjDJPpt9MN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "outputId": "d0eb371e-415f-4b7d-f1b3-a67ae2696b0b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:1535: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([4, 5])) that is different to the input size (torch.Size([4, 1470])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (1470) must match the size of tensor b (5) at non-singleton dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3324749312.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3866\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3868\u001b[0;31m     \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1470) must match the size of tensor b (5) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item()}\"), torch.save(model.state_dict(), \"checkpoint.pth\")"
      ],
      "metadata": {
        "id": "IWVPrPs6yR_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model loss"
      ],
      "metadata": {
        "id": "SQc3O55KwD3Z"
      }
    }
  ]
}
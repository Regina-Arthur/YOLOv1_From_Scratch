{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Regina-Arthur/YOLOv1_From_Scratch/blob/main/YOLOv1_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building a YOLOv1 model from scratch by Regina Arthur\n"
      ],
      "metadata": {
        "id": "E075aKobGM0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###This will be done without the convolutional layers being pretrained on imagenet"
      ],
      "metadata": {
        "id": "2C124Tg5bKL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import the necessary libraries\n"
      ],
      "metadata": {
        "id": "B97rFjkKGMf5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZcJyo6t5oNmj"
      },
      "outputs": [],
      "source": [
        "#Let's import the necessary libraries to build YOLOv1\n",
        "#with pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms.v2 as v2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision.datasets import VOCDetection\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Connect To a GPU if available"
      ],
      "metadata": {
        "id": "OfJGkqRgF28o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's check if cuda is available and connect to it.\n",
        "#If it is not available, use cpu\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kATyTVUZpBC2",
        "outputId": "dd40891e-54da-4749-c8e7-890ab8bd2170"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load the Semantic Boundaries Dataset\n"
      ],
      "metadata": {
        "id": "X2eTGSsJOdtE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a folder to store the Semantic Boundaries Dataset"
      ],
      "metadata": {
        "id": "PMsa5er5doKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #Let's import the necessary libraries for creating a folder in google drive\n",
        "# from google.colab import drive\n",
        "# import os\n",
        "\n",
        "# #Let's mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# def determinedirectory(directory):\n",
        "#   #Let's define folder path in Google Drive\n",
        "#   location = directory.upper()\n",
        "#   root ='/content/drive/MyDrive/_Dataset/'\n",
        "#   new_root = root + location\n",
        "#   #Let's check if the folder exists, if not, let's create it\n",
        "#   if not os.path.exists(new_root):\n",
        "#       os.makedirs(new_root)\n",
        "#       print(f\"Created new folder: {new_root}\")\n",
        "#   else:\n",
        "#       print(f\"Folder already exists: {new_root}\")\n",
        "#   return new_root\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hG5Argrdmr1",
        "outputId": "f6791d89-8d80-4caf-a164-730fb90f8316"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install -q kaggle"
      ],
      "metadata": {
        "id": "pT3cYLdDn-NT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# !kaggle datasets download -d gopalbhattrai/pascal-voc-2012-dataset\n",
        "\n",
        "# Get the directory path in Python\n",
        "# output_dir = determinedirectory(\"PascalVOC2012\")\n",
        "\n",
        "# Now pass that Python variable into the shell command\n",
        "# !unzip \"pascal-voc-2012-dataset.zip\" -d \"$output_dir\""
      ],
      "metadata": {
        "id": "pY2rSKktEY9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Neural Network Architecture\n"
      ],
      "metadata": {
        "id": "ZoJTVgTjpDHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###I am following the documentation on the MODULE class to build my neural network. All neural network model are to inherit from the nn.Module class.\n",
        "\n",
        "####Conv2d\n",
        "####class torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
        "\n",
        "####class torch.nn.LeakyReLU(negative_slope=0.01, inplace=False)\n",
        "\n",
        "####MaxPool2d\n",
        "####class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)[source]\n",
        "\n",
        "####Linear\n",
        "####class torch.nn.Linear(in_features, out_features, bias=True, device=None, dtype=None)[source]\n",
        "\n",
        "####Dropout2d\n",
        "####class torch.nn.Dropout2d(p=0.5, inplace=False)[source]\n",
        "\n"
      ],
      "metadata": {
        "id": "l3MISm9aqDlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOv1(nn.Module):\n",
        "  def __init__(self, S=7, B=2, C=20):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels = 3,out_channels = 64, kernel_size = 7,stride = 2, padding = 3)\n",
        "    self.conv2 = nn.Conv2d(in_channels = 64,out_channels = 192, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv3 = nn.Conv2d(in_channels = 192,out_channels = 128, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv4 = nn.Conv2d(in_channels = 128,out_channels = 256, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv5 = nn.Conv2d(in_channels = 256, out_channels = 256, kernel_size = 1, stride = 1, padding = 0)\n",
        "    self.conv6 = nn.Conv2d(in_channels = 256, out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv7 = nn.Conv2d(in_channels = 512, out_channels = 256, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv8 = nn.Conv2d(in_channels = 256,out_channels = 512, kernel_size = 3, stride = 1, padding = 1)\n",
        "    self.conv9 = nn.Conv2d(in_channels = 512,out_channels = 256, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv10 = nn.Conv2d(in_channels = 256,out_channels = 512, kernel_size = 3,stride = 1, padding =1)\n",
        "    self.conv11 = nn.Conv2d(in_channels = 512,out_channels = 256, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv12 = nn.Conv2d(in_channels = 256,out_channels = 512, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv13 = nn.Conv2d(in_channels = 512,out_channels = 256, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv14 = nn.Conv2d(in_channels = 256,out_channels = 512, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv15 = nn.Conv2d(in_channels = 512,out_channels = 512, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv16 = nn.Conv2d(in_channels = 512,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv17 = nn.Conv2d(in_channels = 1024,out_channels = 512, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv18 = nn.Conv2d(in_channels = 512,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv19 = nn.Conv2d(in_channels = 1024,out_channels = 512, kernel_size = 1,stride = 1, padding = 0)\n",
        "    self.conv20 = nn.Conv2d(in_channels = 512,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv21 = nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv22 = nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3,stride = 2, padding = 1)\n",
        "    self.conv23 = nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "    self.conv24 = nn.Conv2d(in_channels = 1024,out_channels = 1024, kernel_size = 3,stride = 1, padding = 1)\n",
        "\n",
        "    #Pooling and Activation\n",
        "    self.MaxPooling2d = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
        "    self.LeakyReLU = nn.LeakyReLU(negative_slope = 0.1)\n",
        "\n",
        "    #Flattening and Fully Connected Layer\n",
        "    self.Flatten = nn.Flatten()\n",
        "    self.Linear = nn.Linear(in_features = 50176 , out_features = 4096)\n",
        "    self.Dropout = nn.Dropout2d(0.5)\n",
        "    self.Linear2 = nn.Linear(in_features =4096 , out_features = S * S *( B * 5 + 20))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.MaxPooling2d(self.LeakyReLU(self.conv1(x)))\n",
        "    x = self.MaxPooling2d(self.LeakyReLU(self.conv2(x)))\n",
        "    x = self.LeakyReLU(self.conv3(x))\n",
        "    x = self.LeakyReLU(self.conv4(x))\n",
        "    x = self.LeakyReLU(self.conv5(x))\n",
        "    x = self.MaxPooling2d(self.LeakyReLU(self.conv6(x)))\n",
        "    x = self.LeakyReLU(self.conv7(x))\n",
        "    x = self.LeakyReLU(self.conv8(x))\n",
        "    x = self.LeakyReLU(self.conv9(x))\n",
        "    x = self.LeakyReLU(self.conv10(x))\n",
        "    x = self.LeakyReLU(self.conv11(x))\n",
        "    x = self.LeakyReLU(self.conv12(x))\n",
        "    x = self.LeakyReLU(self.conv13(x))\n",
        "    x = self.LeakyReLU(self.conv14(x))\n",
        "    x = self.LeakyReLU(self.conv15(x))\n",
        "    x = self.MaxPooling2d(self.LeakyReLU(self.conv16(x)))\n",
        "    x = self.LeakyReLU(self.conv17(x))\n",
        "    x = self.LeakyReLU(self.conv18(x))\n",
        "    x = self.LeakyReLU(self.conv19(x))\n",
        "    x = self.LeakyReLU(self.conv20(x))\n",
        "    x = self.LeakyReLU(self.conv21(x))\n",
        "    x = self.LeakyReLU(self.conv22(x))\n",
        "    x = self.LeakyReLU(self.conv23(x))\n",
        "    x = self.LeakyReLU(self.conv24(x))\n",
        "    x = self.LeakyReLU(self.Linear(self.Flatten(x)))\n",
        "    x = self.Dropout(x)\n",
        "    x = self.Linear2(x)\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "zjf2zbBvpME-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architecture Test\n",
        "This just makes sure the architecture works as intended\n"
      ],
      "metadata": {
        "id": "PBdPPz0vH-th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = YOLOv1()\n",
        "# model = model.to(device)"
      ],
      "metadata": {
        "id": "cwXfGwm1IVwv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy_input = torch.randn(1, 3, 448, 448).to(device)\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     output = model(dummy_input)\n",
        "\n",
        "# print(\"Output shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2tvjBG5KNa5",
        "outputId": "f99f3132-f9f6-4ce8-dfc0-398b45d9c2df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 1470])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:1535: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# total_params = sum(p.numel() for p in model.parameters())\n",
        "# trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# print(f\"Total params: {total_params:,}\")\n",
        "# print(f\"Trainable params: {trainable_params:,}\")"
      ],
      "metadata": {
        "id": "kAtjUMCyKkv4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a5f213-8ae2-4df9-cd61-4fe3ef2925ef"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total params: 271,703,550\n",
            "Trainable params: 271,703,550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##How to load the Pascal Visual Object Class Dataset\n",
        "\n",
        "Documentation code\n",
        "\n",
        "**class**\n",
        "\n",
        "**classtorchvision.datasets.VOCDetection(root: Union[str, Path], year: str = '2012', image_set: str = 'train', download: bool = False, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, transforms: Optional[Callable] = None)[source]**\n"
      ],
      "metadata": {
        "id": "WdEdF9L5Orvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "model = YOLOv1()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "RJdBbpI3Wf09"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLlkieRy_KEg",
        "outputId": "7205d69d-02ba-439d-c5ec-8dfbf309574f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's reshape the images\n",
        "transforms = v2.Compose([\n",
        "    v2.ColorJitter(brightness=1.5, contrast=1.5, saturation=1.5),\n",
        "    v2.RandomHorizontalFlip(p=0.2),\n",
        "    v2.RandomVerticalFlip(p=0.2),\n",
        "    v2.Resize((448,448)),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "    ])\n",
        "\n",
        "\n",
        "TrainData = torchvision.datasets.VOCDetection(\n",
        "    root = '/content/drive/MyDrive/_Dataset/PASCALVOC2012/VOC2012_test/VOC2012_test',\n",
        "    year = '2008',\n",
        "    image_set = 'train',\n",
        "    download = False,\n",
        "    transform = transforms,\n",
        "    )\n",
        "\n",
        "\n",
        "Traindataloader = DataLoader(TrainData,\n",
        "                             batch_size=64,\n",
        "                             shuffle=True,\n",
        "                             num_workers=2,\n",
        "                             )"
      ],
      "metadata": {
        "id": "4HFih5GnlV2P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "5414f20a-d7d8-47d0-cc96-c11ab346f10c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Dataset not found or corrupted. You can use download=True to download it",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3838717540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m TrainData = torchvision.datasets.VOCDetection(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/_Dataset/PASCALVOC2012/VOC2012_test/VOC2012_test'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'2008'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/voc.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, year, image_set, download, transform, target_transform, transforms)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset not found or corrupted. You can use download=True to download it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0msplits_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ImageSets\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SPLITS_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Dataset not found or corrupted. You can use download=True to download it"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###AdamW\n",
        "class torch.optim.AdamW(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False, *, maximize=False, foreach=None, capturable=False, differentiable=False, fused=None)\n"
      ],
      "metadata": {
        "id": "fsgKHPV3VGV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learning_rate(epoch, epochs):\n",
        "  first_stop = 0.6 * epochs\n",
        "  second_stop = 0.8 * epochs\n",
        "  if epoch <= first_stop:\n",
        "    return 0.1\n",
        "  elif epoch <= second_stop:\n",
        "    return 0.01\n",
        "  else:\n",
        "    return 0.001\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr= 0.1,\n",
        "                              betas=(0.9,0.99),\n",
        "                              weight_decay= 0.0005,\n",
        "                              )\n",
        "# loss = YOLOv1Loss()"
      ],
      "metadata": {
        "id": "vPCDKZ1PVCtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, length=100, img_size=448):\n",
        "        self.length = length\n",
        "        self.img_size = img_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Fake image: random noise\n",
        "        img = torch.rand(3, self.img_size, self.img_size)\n",
        "\n",
        "        # Fake target: one object in YOLO format (class_id, x, y, w, h)\n",
        "        target = torch.tensor([0, 0.5, 0.5, 0.2, 0.2])\n",
        "        return img, target\n",
        "\n",
        "# Create DataLoader\n",
        "dummy_loader = DataLoader(DummyDataset(), batch_size=4, shuffle=True)\n",
        "\n",
        "loss = torch.nn.MSELoss()"
      ],
      "metadata": {
        "id": "bofUZaMK9aPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.param_groups[0]['lr'] = learning_rate(epoch, num_epochs)\n",
        "    for images, targets in Traindataloader:\n",
        "        images, targets = images.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = loss(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "BNJjDJPpt9MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item()}\")\n",
        "    torch.save(model.state_dict(), \"checkpoint.pth\")"
      ],
      "metadata": {
        "id": "IWVPrPs6yR_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model loss"
      ],
      "metadata": {
        "id": "SQc3O55KwD3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class YOLOv1Loss(nn.Module):\n",
        "  def __init__(self, S=7, B=2, C=20, λ_coord=5, λ_noobj=0.5):\n",
        "    super().__init__()"
      ],
      "metadata": {
        "id": "UvbjN5aqQkFs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}